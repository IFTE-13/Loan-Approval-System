{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ec50a69",
   "metadata": {},
   "source": [
    "#### Loan Approval System\n",
    "\n",
    "Author: Mohammed Iftkhar\n",
    "\n",
    "Date: 29th March, 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a00a6a",
   "metadata": {},
   "source": [
    "##### Introduction\n",
    "In the modern financial landscape, loan approval decisions play a crucial role in banking and credit systems. Traditionally, loan approval was based on manual assessment, leading to inefficiencies and potential biases. This project aims to build a Loan Approval System using machine learning (ML) and deep learning techniques to automate, optimize, and enhance the accuracy of loan approval decisions.\n",
    "\n",
    "By leveraging data preprocessing, exploratory data analysis (EDA), feature engineering, feature selection, and multiple ML models, this project provides an intelligent loan approval system capable of making data-driven decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590870c7",
   "metadata": {},
   "source": [
    "##### Description\n",
    "The project follows a systematic approach to data-driven loan approval using machine learning techniques:\n",
    "\n",
    "1. Data Collection & Preprocessing\n",
    "- Load the dataset, clean missing values, and standardize column names.\n",
    "- Convert categorical variables into numerical values.\n",
    "- Handle outliers using statistical techniques.\n",
    "\n",
    "2. Exploratory Data Analysis (EDA)\n",
    "- Perform statistical analysis and visualization to understand the dataset.\n",
    "- Identify key patterns and relationships among loan approval factors.\n",
    "\n",
    "3. Feature Engineering & Selection\n",
    "- Engineer new features such as loan-to-income ratio and asset-to-loan ratio.\n",
    "- Select the most important features using ANOVA F-test to enhance model performance.\n",
    "\n",
    "4. Model Training & Evaluation\n",
    "\n",
    "    âœ… Train multiple machine learning models, including:\n",
    "- Logistic Regression\n",
    "- Decision Tree\n",
    "- Random Forest\n",
    "- Support Vector Machine (SVM)\n",
    "- K-Nearest Neighbors (KNN)\n",
    "- Gradient Boosting\n",
    "\n",
    "    âœ… Use SMOTE to handle class imbalance.\n",
    "\n",
    "    âœ… Evaluate models using accuracy, precision, recall, F1-score, and ROC-AUC curves.\n",
    "\n",
    "5. Neural Network Implementation\n",
    "- Build and train a deep learning model using TensorFlow & Keras.\n",
    "- Implement Batch Normalization, Dropout, and Early Stopping for optimization.\n",
    "\n",
    "6. Model Selection & Deployment\n",
    "- Select the best-performing model (Random Forest in this case).\n",
    "- Save the trained model using joblib for future predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51bd3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c06d31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-learn libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59bfa076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12d9307",
   "metadata": {},
   "source": [
    "##### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "600584c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'warnings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mwarnings\u001b[49m.filterwarnings(\u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Set aesthetic parameters\u001b[39;00m\n\u001b[32m      4\u001b[39m sns.set_style(\u001b[33m\"\u001b[39m\u001b[33mwhitegrid\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'warnings' is not defined"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set aesthetic parameters\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 8)\n",
    "plt.rcParams[\"font.size\"] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7dc9371",
   "metadata": {},
   "source": [
    "##### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c345627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and inspect dataset\n",
    "def load_data():\n",
    "    df = pd.read_csv(\"data.csv\")\n",
    "    print(\"Dataset loaded successfully!\")\n",
    "    print(f\"\\nShape: {df.shape}\")\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    display(df.head())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0894bda",
   "metadata": {},
   "source": [
    "##### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0e6991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning and Preprocessing\n",
    "def clean_data(df):\n",
    "    # Make column names consistent\n",
    "    df.columns = df.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
    "\n",
    "    # Handle missing values\n",
    "    print(\"\\nMissing values before cleaning:\")\n",
    "    print(df.isnull().sum())\n",
    "\n",
    "    # Drop rows with missing values (alternative: imputation could be used)\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    # Convert data types\n",
    "    numeric_cols = [\n",
    "        \"no_of_dependents\",\n",
    "        \"income_annum\",\n",
    "        \"loan_amount\",\n",
    "        \"loan_term\",\n",
    "        \"cibil_score\",\n",
    "        \"residential_assets_value\",\n",
    "        \"commercial_assets_value\",\n",
    "        \"luxury_assets_value\",\n",
    "        \"bank_asset_value\",\n",
    "    ]\n",
    "\n",
    "    for col in numeric_cols:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\").astype(\"int64\")\n",
    "\n",
    "    print(\"\\nMissing values after cleaning:\")\n",
    "    print(df.isnull().sum())\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4682d350",
   "metadata": {},
   "source": [
    "##### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dfdd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_eda(df):\n",
    "    print(\"\\nBasic Statistics:\")\n",
    "    display(df.describe())\n",
    "\n",
    "    print(\"\\nCategorical Features Distribution:\")\n",
    "    cat_cols = [\"education\", \"self_employed\", \"loan_status\"]\n",
    "    for col in cat_cols:\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        sns.countplot(data=df, x=col, palette=\"viridis\")\n",
    "        plt.title(f\"Distribution of {col}\")\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.show()\n",
    "        print(df[col].value_counts(normalize=True))\n",
    "\n",
    "    # Correlation analysis\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    corr = df.corr(numeric_only=True)\n",
    "    sns.heatmap(corr, annot=True, cmap=\"coolwarm\", center=0, fmt=\".2f\")\n",
    "    plt.title(\"Feature Correlation Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "    # Identify highly correlated features\n",
    "    high_corr = []\n",
    "    for col in corr.columns:\n",
    "        high_corr_count = (corr[col].abs() > 0.8).sum() - 1  # Exclude self-correlation\n",
    "        if high_corr_count >= 1:\n",
    "            high_corr.append(col)\n",
    "\n",
    "    print(\"\\nFeatures with high correlation (>0.8):\", high_corr)\n",
    "\n",
    "    # Visualize numeric features with dynamic layout\n",
    "    numeric_features = df.select_dtypes(include=[\"int64\"]).columns\n",
    "    num_features = len(numeric_features)\n",
    "\n",
    "    # Calculate optimal grid size\n",
    "    cols = 3\n",
    "    rows = (num_features // cols) + (1 if num_features % cols else 0)\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(15, 5 * rows))\n",
    "    axes = axes.flatten()  # Flatten in case it's a 2D array\n",
    "\n",
    "    for i, col in enumerate(numeric_features):\n",
    "        if i < len(axes):  # Ensure we don't exceed the number of subplots\n",
    "            sns.histplot(data=df, x=col, bins=20, kde=True, ax=axes[i])\n",
    "            axes[i].set_title(f\"Distribution of {col}\")\n",
    "\n",
    "    # Hide any empty subplots\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    plt.suptitle(\"Distribution of Numeric Features\", y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108e7cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "def engineer_features(df):\n",
    "    # Drop loan_id as it's just an identifier\n",
    "    df.drop(\"loan_id\", axis=1, inplace=True)\n",
    "\n",
    "    # Create new features\n",
    "    df[\"total_assets\"] = (\n",
    "        df[\"residential_assets_value\"]\n",
    "        + df[\"commercial_assets_value\"]\n",
    "        + df[\"luxury_assets_value\"]\n",
    "        + df[\"bank_asset_value\"]\n",
    "    )\n",
    "\n",
    "    df[\"loan_to_income_ratio\"] = df[\"loan_amount\"] / df[\"income_annum\"]\n",
    "    df[\"asset_to_loan_ratio\"] = df[\"total_assets\"] / df[\"loan_amount\"]\n",
    "\n",
    "    # Encode categorical features\n",
    "    le = LabelEncoder()\n",
    "    df[\"education\"] = le.fit_transform(df[\"education\"])\n",
    "    df[\"self_employed\"] = le.fit_transform(df[\"self_employed\"])\n",
    "    df[\"loan_status\"] = le.fit_transform(df[\"loan_status\"])\n",
    "\n",
    "    # Handle outliers using winsorization\n",
    "    numeric_cols = df.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "    for col in numeric_cols:\n",
    "        q1 = df[col].quantile(0.05)\n",
    "        q3 = df[col].quantile(0.95)\n",
    "        df[col] = np.where(df[col] < q1, q1, df[col])\n",
    "        df[col] = np.where(df[col] > q3, q3, df[col])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2e360d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection\n",
    "def select_features(df):\n",
    "    X = df.drop(\"loan_status\", axis=1)\n",
    "    y = df[\"loan_status\"]\n",
    "\n",
    "    # Feature selection using ANOVA F-value\n",
    "    selector = SelectKBest(f_classif, k=\"all\")\n",
    "    selector.fit(X, y)\n",
    "\n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    feature_scores = pd.DataFrame({\"Feature\": X.columns, \"Score\": selector.scores_})\n",
    "    feature_scores = feature_scores.sort_values(\"Score\", ascending=False)\n",
    "\n",
    "    sns.barplot(data=feature_scores, x=\"Score\", y=\"Feature\", palette=\"viridis\")\n",
    "    plt.title(\"Feature Importance Scores (ANOVA F-value)\")\n",
    "    plt.xlabel(\"F-value\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Select top features\n",
    "    selected_features = feature_scores[feature_scores[\"Score\"] > 10][\"Feature\"].tolist()\n",
    "    print(\"\\nSelected features:\", selected_features)\n",
    "\n",
    "    return df[selected_features + [\"loan_status\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b3d76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training and Evaluation\n",
    "def train_models(df):\n",
    "    X = df.drop(\"loan_status\", axis=1)\n",
    "    y = df[\"loan_status\"]\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    # Handle class imbalance\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Define models\n",
    "    models = {\n",
    "        \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "        \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "        \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "        \"SVM\": SVC(probability=True, random_state=42),\n",
    "        \"KNN\": KNeighborsClassifier(),\n",
    "        \"Gradient Boosting\": GradientBoostingClassifier(random_state=42),\n",
    "    }\n",
    "\n",
    "    # Train and evaluate models\n",
    "    results = []\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_prob = (\n",
    "            model.predict_proba(X_test)[:, 1]\n",
    "            if hasattr(model, \"predict_proba\")\n",
    "            else [0] * len(y_test)\n",
    "        )\n",
    "\n",
    "        # Metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        roc_auc = roc_auc_score(y_test, y_prob) if any(y_prob) else 0\n",
    "        report = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"Model\": name,\n",
    "                \"Accuracy\": accuracy,\n",
    "                \"Precision\": report[\"weighted avg\"][\"precision\"],\n",
    "                \"Recall\": report[\"weighted avg\"][\"recall\"],\n",
    "                \"F1-Score\": report[\"weighted avg\"][\"f1-score\"],\n",
    "                \"ROC AUC\": roc_auc,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Plot confusion matrix\n",
    "        plt.figure(figsize=(6, 5))\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        sns.heatmap(\n",
    "            cm,\n",
    "            annot=True,\n",
    "            fmt=\"d\",\n",
    "            cmap=\"Blues\",\n",
    "            xticklabels=[\"Rejected\", \"Approved\"],\n",
    "            yticklabels=[\"Rejected\", \"Approved\"],\n",
    "        )\n",
    "        plt.title(f\"{name} Confusion Matrix\")\n",
    "        plt.ylabel(\"Actual\")\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.show()\n",
    "\n",
    "        # Plot ROC curve if applicable\n",
    "        if any(y_prob):\n",
    "            fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.plot(fpr, tpr, label=f\"{name} (AUC = {roc_auc:.2f})\")\n",
    "            plt.plot([0, 1], [0, 1], \"k--\")\n",
    "            plt.xlabel(\"False Positive Rate\")\n",
    "            plt.ylabel(\"True Positive Rate\")\n",
    "            plt.title(\"ROC Curve\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "    # Display results\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df = results_df.sort_values(\"Accuracy\", ascending=False)\n",
    "\n",
    "    print(\"\\nModel Performance Comparison:\")\n",
    "    display(\n",
    "        results_df.style.background_gradient(\n",
    "            cmap=\"viridis\", subset=[\"Accuracy\", \"ROC AUC\"]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Plot model comparison\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    results_df.set_index(\"Model\").plot(\n",
    "        kind=\"bar\",\n",
    "        y=[\"Accuracy\", \"F1-Score\", \"ROC AUC\"],\n",
    "        color=[\"#3498db\", \"#2ecc71\", \"#9b59b6\"],\n",
    "    )\n",
    "    plt.title(\"Model Performance Comparison\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylim(0.5, 1.0)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return results_df, X_train, X_test, y_train, y_test, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195c53c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network Model\n",
    "def build_nn(X_train, y_train, X_test, y_test):\n",
    "    model = Sequential(\n",
    "        [\n",
    "            Dense(128, activation=\"relu\", input_shape=(X_train.shape[1],)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            Dense(64, activation=\"relu\"),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            Dense(32, activation=\"relu\"),\n",
    "            Dense(1, activation=\"sigmoid\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\", tf.keras.metrics.AUC()],\n",
    "    )\n",
    "\n",
    "    early_stop = EarlyStopping(\n",
    "        monitor=\"val_loss\", patience=5, restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        validation_data=(X_test, y_test),\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        callbacks=[early_stop],\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history[\"accuracy\"], label=\"Train Accuracy\")\n",
    "    plt.plot(history.history[\"val_accuracy\"], label=\"Validation Accuracy\")\n",
    "    plt.title(\"Model Accuracy\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history[\"loss\"], label=\"Train Loss\")\n",
    "    plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "    plt.title(\"Model Loss\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Evaluate model\n",
    "    y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "    print(\"\\nNeural Network Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5f093b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "def main():\n",
    "    # Load and preprocess data\n",
    "    df = load_data()\n",
    "    df = clean_data(df)\n",
    "    df = perform_eda(df)\n",
    "    df = engineer_features(df)\n",
    "    df = select_features(df)\n",
    "\n",
    "    # Train traditional ML models\n",
    "    results_df, X_train, X_test, y_train, y_test, scaler = train_models(df)\n",
    "\n",
    "    # Train neural network\n",
    "    # nn_model = build_nn(X_train, y_train, X_test, y_test)\n",
    "\n",
    "    # Save best model (Random Forest in this case)\n",
    "    best_model = RandomForestClassifier(random_state=42)\n",
    "    best_model.fit(\n",
    "        scaler.fit_transform(df.drop(\"loan_status\", axis=1)), df[\"loan_status\"]\n",
    "    )\n",
    "\n",
    "    import joblib\n",
    "\n",
    "    joblib.dump(best_model, \"loan_approval_model.pkl\")\n",
    "    joblib.dump(scaler, \"scaler.pkl\")\n",
    "    print(\"\\nBest model saved as 'loan_approval_model.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eebe57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1baa46",
   "metadata": {},
   "source": [
    "##### Conclusion\n",
    "This project successfully demonstrates the effectiveness of machine learning and deep learning in loan approval automation. Through thorough data preprocessing, feature engineering, and model evaluation, the system achieves high accuracy and reliable performance in predicting loan approvals.\n",
    "\n",
    "Among all the models tested, the Random Forest Classifier provided the best performance, making it the ideal choice for deployment. Additionally, the deep learning model showed promising results, indicating the potential for further improvements.\n",
    "\n",
    "By automating loan approvals, this system can:\n",
    "\n",
    "âœ… Improve decision-making speed\n",
    "\n",
    "âœ… Enhance accuracy & reduce human bias\n",
    "\n",
    "âœ… Optimize risk assessment for banks & financial institutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60329789",
   "metadata": {},
   "source": [
    "##### Future Work\n",
    "Although this project achieves high accuracy, there is always room for improvement. Some areas for future enhancements include:\n",
    "\n",
    "ðŸ”¹ Hyperparameter Tuning: Further fine-tuning ML and deep learning models using techniques like Bayesian Optimization and Grid Search.\n",
    "\n",
    "ðŸ”¹ Explainability & Interpretability: Implement SHAP and LIME to interpret model decisions transparently.\n",
    "\n",
    "ðŸ”¹ Integration with Real-Time Systems: Deploy the model as a web application (Flask/Django) or integrate it into a banking system for real-time predictions.\n",
    "\n",
    "ðŸ”¹ Advanced Deep Learning Architectures: Explore more sophisticated models like LSTM (Long Short-Term Memory) for sequential data processing.\n",
    "\n",
    "ðŸ”¹ Improved Data Handling: Use automated data pipelines and integrate the system with real-world financial datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
